{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Trends Prediction - LightGBM Model\n",
    "\n",
    "This notebook trains a LightGBM model to predict daily interest values for Google Trends data.\n",
    "\n",
    "**Algorithm**: LightGBM (Light Gradient Boosting Machine)\n",
    "- Fast training speed\n",
    "- Low memory usage\n",
    "- High accuracy\n",
    "- Handles categorical features natively\n",
    "\n",
    "**Target**: Predict interest_value (0-100) for next day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed daily trends data\n",
    "# Replace with your actual processed file path\n",
    "data_path = '../data/processed/daily_trends_processed_latest.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['keyword', 'category', 'date'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Keywords: {df['keyword'].nunique()}\")\n",
    "print(f\"Categories: {df['category'].nunique()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time-based and lag features for LightGBM\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Lag features (previous days)\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:\n",
    "        df[f'lag_{lag}'] = df.groupby(['keyword', 'category'])['interest_value'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby(['keyword', 'category'])['interest_value'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f'rolling_std_{window}'] = df.groupby(['keyword', 'category'])['interest_value'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # Trend features\n",
    "    df['diff_1'] = df.groupby(['keyword', 'category'])['interest_value'].diff(1)\n",
    "    df['diff_7'] = df.groupby(['keyword', 'category'])['interest_value'].diff(7)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "df = create_features(df)\n",
    "\n",
    "# Drop rows with NaN (from lag features)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Features created. New shape: {df.shape}\")\n",
    "print(f\"Feature columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = [\n",
    "    'day', 'month', 'year', 'dayofweek', 'quarter', 'weekofyear',\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_7', 'lag_14', 'lag_30',\n",
    "    'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_30',\n",
    "    'rolling_std_7', 'rolling_std_14', 'rolling_std_30',\n",
    "    'diff_1', 'diff_7'\n",
    "]\n",
    "\n",
    "categorical_features = ['keyword', 'category', 'dayofweek', 'month', 'quarter']\n",
    "\n",
    "# Encode categorical features\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Time-based split (80% train, 20% test)\n",
    "split_date = df['date'].quantile(0.8)\n",
    "\n",
    "train_df = df[df['date'] < split_date]\n",
    "test_df = df[df['date'] >= split_date]\n",
    "\n",
    "X_train = train_df[feature_cols + categorical_features]\n",
    "y_train = train_df['interest_value']\n",
    "\n",
    "X_test = test_df[feature_cols + categorical_features]\n",
    "y_test = test_df['interest_value']\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train period: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Test period: {test_df['date'].min()} to {test_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data, categorical_feature=categorical_features)\n",
    "\n",
    "# Train model\n",
    "print(\"Training LightGBM model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'test'],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "print(f\"Best score: {model.best_score['test']['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R2:   {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_pred_train, \"Train\")\n",
    "test_metrics = calculate_metrics(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': model.feature_name(),\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df.head(20), x='importance', y='feature')\n",
    "plt.title('Top 20 Feature Importance (LightGBM)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Train set\n",
    "axes[0].scatter(y_train, y_pred_train, alpha=0.3, s=10)\n",
    "axes[0].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Interest Value')\n",
    "axes[0].set_ylabel('Predicted Interest Value')\n",
    "axes[0].set_title(f'Train Set (R2 = {train_metrics[\"R2\"]:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_pred_test, alpha=0.3, s=10, color='orange')\n",
    "axes[1].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Interest Value')\n",
    "axes[1].set_ylabel('Predicted Interest Value')\n",
    "axes[1].set_title(f'Test Set (R2 = {test_metrics[\"R2\"]:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot for a sample keyword\n",
    "sample_keyword = test_df['keyword'].iloc[0]\n",
    "sample_category = test_df['category'].iloc[0]\n",
    "\n",
    "sample_data = test_df[\n",
    "    (test_df['keyword'] == sample_keyword) & \n",
    "    (test_df['category'] == sample_category)\n",
    "].copy()\n",
    "\n",
    "sample_data['predicted'] = model.predict(\n",
    "    sample_data[feature_cols + categorical_features],\n",
    "    num_iteration=model.best_iteration\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(sample_data['date'], sample_data['interest_value'], label='Actual', marker='o', linewidth=2)\n",
    "plt.plot(sample_data['date'], sample_data['predicted'], label='Predicted', marker='s', linewidth=2, alpha=0.7)\n",
    "plt.title(f'Predictions for \"{sample_keyword}\" ({sample_category})', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Interest Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = '../models/lightgbm_daily_model.txt'\n",
    "model.save_model(model_path)\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'train': train_metrics,\n",
    "    'test': test_metrics,\n",
    "    'feature_importance': importance_df.to_dict('records')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/lightgbm_daily_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Metrics saved to: ../models/lightgbm_daily_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Model Performance:\n",
    "- Algorithm: LightGBM\n",
    "- Test RMSE: Check output above\n",
    "- Test R2: Check output above\n",
    "- Training Time: Fast (typically under 1 minute)\n",
    "\n",
    "### Next Steps:\n",
    "1. Try hyperparameter tuning with Optuna\n",
    "2. Add more features (holidays, events, etc.)\n",
    "3. Ensemble with other models\n",
    "4. Deploy for production inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
