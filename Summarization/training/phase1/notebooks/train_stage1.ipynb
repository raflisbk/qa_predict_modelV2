{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:32:49.464061Z",
     "iopub.status.busy": "2026-01-16T11:32:49.463820Z",
     "iopub.status.idle": "2026-01-16T11:32:59.342328Z",
     "shell.execute_reply": "2026-01-16T11:32:59.341546Z",
     "shell.execute_reply.started": "2026-01-16T11:32:49.464025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Disable Unsloth interference and install dependencies.\"\"\"\n",
    "import os\n",
    "\n",
    "# Disable torch compile to avoid Unsloth interference\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q emoji rouge-score huggingface_hub\n",
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:32:59.343976Z",
     "iopub.status.busy": "2026-01-16T11:32:59.343699Z",
     "iopub.status.idle": "2026-01-16T11:33:08.802388Z",
     "shell.execute_reply": "2026-01-16T11:33:08.801603Z",
     "shell.execute_reply.started": "2026-01-16T11:32:59.343946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 11:33:05.455265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768563185.477604     147 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768563185.484501     147 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768563185.501665     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768563185.501687     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768563185.501689     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768563185.501691     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Import libraries and configure environment.\"\"\"\n",
    "import re\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:08.805064Z",
     "iopub.status.busy": "2026-01-16T11:33:08.804482Z",
     "iopub.status.idle": "2026-01-16T11:33:08.810206Z",
     "shell.execute_reply": "2026-01-16T11:33:08.809418Z",
     "shell.execute_reply.started": "2026-01-16T11:33:08.805025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Configuration constants for training.\"\"\"\n",
    "\n",
    "HF_TOKEN = \"\"  \n",
    "HF_USERNAME = \"\"\n",
    "HF_REPO_NAME = \"\"git \n",
    "\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/suummarization-data/training_dataset_openrouter.csv\"\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = \"google/flan-t5-large\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "# LoRA settings\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 128\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 4  # Reduced for stability\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 20\n",
    "SAVE_STEPS = 173\n",
    "EVAL_STEPS = 173\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "# Computed values\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/{HF_REPO_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:08.811646Z",
     "iopub.status.busy": "2026-01-16T11:33:08.811248Z",
     "iopub.status.idle": "2026-01-16T11:33:09.037236Z",
     "shell.execute_reply": "2026-01-16T11:33:09.036639Z",
     "shell.execute_reply.started": "2026-01-16T11:33:08.811621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Login to HuggingFace Hub.\"\"\"\n",
    "login(token=HF_TOKEN)\n",
    "logger.info(f\"Logged in to HuggingFace Hub as {HF_USERNAME}\")\n",
    "logger.info(f\"Model will be pushed to: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:09.038339Z",
     "iopub.status.busy": "2026-01-16T11:33:09.038067Z",
     "iopub.status.idle": "2026-01-16T11:33:09.046965Z",
     "shell.execute_reply": "2026-01-16T11:33:09.046142Z",
     "shell.execute_reply.started": "2026-01-16T11:33:09.038315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Text preprocessing functions.\"\"\"\n",
    "\n",
    "\n",
    "def remove_markdown_formatting(text: str) -> str:\n",
    "    \"\"\"Remove markdown bold and italic formatting.\"\"\"\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"__(.+?)__\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\*(.+?)\\*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"_(.+?)_\", r\"\\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    \"\"\"Remove emojis from text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    \"\"\"Normalize unicode characters to ASCII equivalents.\"\"\"\n",
    "    replacements = {\n",
    "        \"â€”\": \"-\", \"â€“\": \"-\", \"'\": \"'\", \"'\": \"'\",\n",
    "        \"\\\"\": '\"', \"\\\"\": '\"', \"â€¦\": \"...\",\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_whitespace(text: str) -> str:\n",
    "    \"\"\"Clean excessive whitespace and newlines.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Apply all text cleaning steps.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_markdown_formatting(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = normalize_unicode(text)\n",
    "    text = clean_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:09.048222Z",
     "iopub.status.busy": "2026-01-16T11:33:09.047908Z",
     "iopub.status.idle": "2026-01-16T11:33:09.070694Z",
     "shell.execute_reply": "2026-01-16T11:33:09.069899Z",
     "shell.execute_reply.started": "2026-01-16T11:33:09.048187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Test Results:\n",
      "==================================================\n",
      "Input:  **STRIKE NOW!** ðŸš€ Post at 7PMâ€”don't miss it!\n",
      "Output: STRIKE NOW! Post at 7PM-don't miss it!\n",
      "--------------------------------------------------\n",
      "Input:  This is __important__ and *urgent* info.\n",
      "Output: This is important and urgent info.\n",
      "--------------------------------------------------\n",
      "Input:  Peak engagement... score: 95â€”the best!\n",
      "Output: Peak engagement... score: 95-the best!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test preprocessing functions.\"\"\"\n",
    "\n",
    "test_cases = [\n",
    "    \"**STRIKE NOW!** ðŸš€ Post at 7PMâ€”don't miss it!\",\n",
    "    \"This is __important__ and *urgent* info.\",\n",
    "    \"Peak engagement... score: 95â€”the best!\",\n",
    "]\n",
    "\n",
    "print(\"Preprocessing Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "for test in test_cases:\n",
    "    cleaned = clean_text(test)\n",
    "    print(f\"Input:  {test}\")\n",
    "    print(f\"Output: {cleaned}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:09.072189Z",
     "iopub.status.busy": "2026-01-16T11:33:09.071725Z",
     "iopub.status.idle": "2026-01-16T11:33:10.993527Z",
     "shell.execute_reply": "2026-01-16T11:33:10.992798Z",
     "shell.execute_reply.started": "2026-01-16T11:33:09.072164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data:\n",
      "Input: Day: Monday, Time: 09:00 - 12:00, Score: 97, Dominance: Unrivaled, Shape: Sustained Plateau, Style: Storytelling (Narrative flow)\n",
      "Target: Monday mid-mornings capture professionals settling into their workweek, seeking ...\n",
      "--------------------------------------------------\n",
      "Input: Day: Thursday, Time: 13:00 - 17:00, Score: 85, Dominance: Unrivaled, Shape: Sustained Plateau, Style: Strategic (Focus on ROI & Growth)\n",
      "Target: Thursday afternoon (13:00-17:00) delivers exceptional ROI with an 85/100 score-n...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load and preprocess the dataset.\"\"\"\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV and apply preprocessing.\"\"\"\n",
    "    logger.info(f\"Loading data from {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    original_count = len(df)\n",
    "    logger.info(f\"Loaded {original_count} rows\")\n",
    "\n",
    "    # Apply preprocessing\n",
    "    logger.info(\"Applying text preprocessing...\")\n",
    "    df[\"input_text\"] = df[\"input_text\"].apply(clean_text)\n",
    "    df[\"target_text\"] = df[\"target_text\"].apply(clean_text)\n",
    "\n",
    "    # Remove empty rows\n",
    "    df = df[\n",
    "        (df[\"input_text\"].str.len() > 0) &\n",
    "        (df[\"target_text\"].str.len() > 0)\n",
    "    ]\n",
    "\n",
    "    cleaned_count = len(df)\n",
    "    logger.info(f\"After cleaning: {cleaned_count} rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = load_and_preprocess_data(DATA_PATH)\n",
    "\n",
    "# Display samples\n",
    "print(\"\\nSample Data:\")\n",
    "for i in range(2):\n",
    "    print(f\"Input: {df.iloc[i]['input_text']}\")\n",
    "    print(f\"Target: {df.iloc[i]['target_text'][:80]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:10.994778Z",
     "iopub.status.busy": "2026-01-16T11:33:10.994524Z",
     "iopub.status.idle": "2026-01-16T11:33:11.032167Z",
     "shell.execute_reply": "2026-01-16T11:33:11.031531Z",
     "shell.execute_reply.started": "2026-01-16T11:33:10.994753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 5508\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 611\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create train/validation split.\"\"\"\n",
    "\n",
    "\n",
    "def create_dataset_splits(\n",
    "    df: pd.DataFrame,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: int = 42\n",
    ") -> DatasetDict:\n",
    "    \"\"\"Split data into train and validation sets.\"\"\"\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    val_size = int(len(df) * val_ratio)\n",
    "    train_df = df[val_size:]\n",
    "    val_df = df[:val_size]\n",
    "\n",
    "    logger.info(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df[[\"input_text\", \"target_text\"]])\n",
    "    val_dataset = Dataset.from_pandas(val_df[[\"input_text\", \"target_text\"]])\n",
    "\n",
    "    return DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "\n",
    "\n",
    "dataset = create_dataset_splits(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model & Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:11.033338Z",
     "iopub.status.busy": "2026-01-16T11:33:11.033102Z",
     "iopub.status.idle": "2026-01-16T11:33:12.888844Z",
     "shell.execute_reply": "2026-01-16T11:33:12.888242Z",
     "shell.execute_reply.started": "2026-01-16T11:33:11.033314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load tokenizer.\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "logger.info(f\"Tokenizer loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:12.890059Z",
     "iopub.status.busy": "2026-01-16T11:33:12.889800Z",
     "iopub.status.idle": "2026-01-16T11:33:35.008344Z",
     "shell.execute_reply": "2026-01-16T11:33:35.007741Z",
     "shell.execute_reply.started": "2026-01-16T11:33:12.890034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b505c6b469b42f98e0f1f8acffe3e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae39656fabf48d8bfecaf931fc9e153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:1598: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load model with 4-bit quantization (QLoRA).\"\"\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=False  # Disable to avoid Unsloth conflict\n",
    ")\n",
    "\n",
    "logger.info(f\"Model loaded with 4-bit quantization: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:35.009619Z",
     "iopub.status.busy": "2026-01-16T11:33:35.009316Z",
     "iopub.status.idle": "2026-01-16T11:33:35.777040Z",
     "shell.execute_reply": "2026-01-16T11:33:35.776394Z",
     "shell.execute_reply.started": "2026-01-16T11:33:35.009584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 37,748,736 || all params: 820,898,816 || trainable%: 4.5985\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Configure and apply LoRA.\"\"\"\n",
    "\n",
    "# LoRA configuration for T5\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q\", \"v\", \"k\", \"o\"],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:35.780009Z",
     "iopub.status.busy": "2026-01-16T11:33:35.779560Z",
     "iopub.status.idle": "2026-01-16T11:33:49.939743Z",
     "shell.execute_reply": "2026-01-16T11:33:49.939103Z",
     "shell.execute_reply.started": "2026-01-16T11:33:35.779983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec42da99fe343869348c2db6deb5700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/5508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3b6f53f81d45b4aa4698e43676c915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5508\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 611\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization functions.\"\"\"\n",
    "\n",
    "T5_PREFIX = \"summarize : \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples: Dict) -> Dict:\n",
    "    \"\"\"Tokenize input and target text for T5.\"\"\"\n",
    "    inputs = [T5_PREFIX + text for text in examples[\"input_text\"]]\n",
    "    targets = examples[\"target_text\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in ids]\n",
    "        for ids in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "logger.info(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"target_text\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:49.940837Z",
     "iopub.status.busy": "2026-01-16T11:33:49.940611Z",
     "iopub.status.idle": "2026-01-16T11:33:49.947920Z",
     "shell.execute_reply": "2026-01-16T11:33:49.947278Z",
     "shell.execute_reply.started": "2026-01-16T11:33:49.940813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Reset training dari awal - Hapus lokal + Hub.\n",
    "\n",
    "JALANKAN CELL INI HANYA JIKA INGIN RESET TOTAL!\n",
    "Set RESET_TRAINING = True untuk reset, False untuk skip.\n",
    "\"\"\"\n",
    "import shutil\n",
    "from huggingface_hub import HfApi, list_repo_files\n",
    "\n",
    "RESET_TRAINING = False  # Set True untuk reset, False untuk skip\n",
    "\n",
    "\n",
    "def reset_local_checkpoints(output_dir: str) -> None:\n",
    "    \"\"\"Hapus semua checkpoint lokal.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Path ke direktori checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "        logger.info(\"Local checkpoints deleted\")\n",
    "    else:\n",
    "        logger.info(\"No local checkpoints to delete\")\n",
    "\n",
    "\n",
    "def reset_hub_repository(repo_id: str, token: str) -> None:\n",
    "    \"\"\"Hapus semua files di HuggingFace Hub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_id: ID repository di Hub (username/repo-name).\n",
    "        token: HuggingFace API token.\n",
    "    \"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    try:\n",
    "        files = list_repo_files(repo_id, token=token)\n",
    "        for file in files:\n",
    "            if file != \".gitattributes\":\n",
    "                api.delete_file(\n",
    "                    path_in_repo=file,\n",
    "                    repo_id=repo_id,\n",
    "                    token=token\n",
    "                )\n",
    "                logger.info(f\"Deleted from Hub: {file}\")\n",
    "        logger.info(\"HuggingFace Hub reset complete\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Hub reset skipped: {e}\")\n",
    "\n",
    "\n",
    "if RESET_TRAINING:\n",
    "    reset_local_checkpoints(OUTPUT_DIR)\n",
    "    reset_hub_repository(HF_MODEL_ID, HF_TOKEN)\n",
    "    logger.info(\"Training will start from step 0\")\n",
    "else:\n",
    "    logger.info(\"Reset skipped - will resume from checkpoint if available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:49.949184Z",
     "iopub.status.busy": "2026-01-16T11:33:49.948781Z",
     "iopub.status.idle": "2026-01-16T11:33:49.963560Z",
     "shell.execute_reply": "2026-01-16T11:33:49.962870Z",
     "shell.execute_reply.started": "2026-01-16T11:33:49.949160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Auto-load checkpoint dari HuggingFace Hub jika lokal kosong.\n",
    "\n",
    "Ini memastikan training bisa resume dengan benar setelah Kaggle session restart.\n",
    "\"\"\"\n",
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "\n",
    "\n",
    "def get_local_checkpoints(output_dir: str) -> list:\n",
    "    \"\"\"Get list of local checkpoint directories.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Path ke direktori checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        List of checkpoint directory names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return []\n",
    "    return [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "\n",
    "\n",
    "def sync_from_hub(\n",
    "    repo_id: str,\n",
    "    output_dir: str,\n",
    "    token: str\n",
    ") -> None:\n",
    "    \"\"\"Download model files dari Hub jika lokal kosong.\n",
    "\n",
    "    Args:\n",
    "        repo_id: ID repository di Hub.\n",
    "        output_dir: Path untuk menyimpan model.\n",
    "        token: HuggingFace API token.\n",
    "    \"\"\"\n",
    "    local_checkpoints = get_local_checkpoints(output_dir)\n",
    "\n",
    "    if local_checkpoints:\n",
    "        logger.info(f\"Found {len(local_checkpoints)} local checkpoints\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        hub_files = list_repo_files(repo_id, token=token)\n",
    "        if \"adapter_model.safetensors\" in hub_files:\n",
    "            logger.info(\"Downloading model from HuggingFace Hub...\")\n",
    "            snapshot_download(\n",
    "                repo_id=repo_id,\n",
    "                local_dir=output_dir,\n",
    "                token=token,\n",
    "                ignore_patterns=[\"*.md\", \"*.txt\"],\n",
    "            )\n",
    "            logger.info(f\"Model downloaded to: {output_dir}\")\n",
    "        else:\n",
    "            logger.info(\"No model found on Hub - starting fresh\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not sync from Hub: {e}\")\n",
    "        logger.info(\"Starting fresh training\")\n",
    "\n",
    "\n",
    "sync_from_hub(HF_MODEL_ID, OUTPUT_DIR, HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:49.964862Z",
     "iopub.status.busy": "2026-01-16T11:33:49.964560Z",
     "iopub.status.idle": "2026-01-16T11:33:50.008677Z",
     "shell.execute_reply": "2026-01-16T11:33:50.007810Z",
     "shell.execute_reply.started": "2026-01-16T11:33:49.964838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Configure training arguments dengan learning rate scheduler.\"\"\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # Output & Hub\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HF_MODEL_ID,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_token=HF_TOKEN,\n",
    "\n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine decay - smooth decrease\n",
    "    warmup_ratio=0.1,            # 10% steps untuk warmup\n",
    "\n",
    "    # Precision\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "\n",
    "    # Saving & Evaluation\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "\n",
    "    # Gradient\n",
    "    gradient_checkpointing=False,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "logger.info(\"Training arguments configured with cosine LR scheduler\")\n",
    "logger.info(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "logger.info(f\"Warmup: 10% of total steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:50.010196Z",
     "iopub.status.busy": "2026-01-16T11:33:50.009905Z",
     "iopub.status.idle": "2026-01-16T11:33:50.016831Z",
     "shell.execute_reply": "2026-01-16T11:33:50.016064Z",
     "shell.execute_reply.started": "2026-01-16T11:33:50.010171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Check for existing checkpoint to resume from.\"\"\"\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    \"\"\"Find the latest checkpoint directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "\n",
    "    checkpoints = [\n",
    "        d for d in os.listdir(output_dir)\n",
    "        if d.startswith(\"checkpoint-\")\n",
    "    ]\n",
    "\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "    latest = os.path.join(output_dir, checkpoints[-1])\n",
    "    return latest\n",
    "\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(OUTPUT_DIR)\n",
    "if latest_checkpoint:\n",
    "    logger.info(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "else:\n",
    "    logger.info(\"No checkpoint found. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:50.018238Z",
     "iopub.status.busy": "2026-01-16T11:33:50.017843Z",
     "iopub.status.idle": "2026-01-16T11:33:50.236539Z",
     "shell.execute_reply": "2026-01-16T11:33:50.235938Z",
     "shell.execute_reply.started": "2026-01-16T11:33:50.018202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147/3246972690.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialize trainer.\"\"\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:50.237829Z",
     "iopub.status.busy": "2026-01-16T11:33:50.237439Z",
     "iopub.status.idle": "2026-01-16T11:33:51.663096Z",
     "shell.execute_reply": "2026-01-16T11:33:51.662511Z",
     "shell.execute_reply.started": "2026-01-16T11:33:50.237792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3460' max='3460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3460/3460 : < :, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \"\"\"Start or resume training.\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"Starting Training\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Model: {MODEL_NAME}\")\n",
    "    logger.info(f\"Train: {len(tokenized_dataset['train'])}, Val: {len(tokenized_dataset['validation'])}\")\n",
    "    logger.info(f\"Epochs: {NUM_EPOCHS}, Hub: {HF_MODEL_ID}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = train_result.metrics\n",
    "    logger.info(f\"Training completed. Final metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:33:51.664341Z",
     "iopub.status.busy": "2026-01-16T11:33:51.664005Z",
     "iopub.status.idle": "2026-01-16T11:35:05.973120Z",
     "shell.execute_reply": "2026-01-16T11:35:05.972469Z",
     "shell.execute_reply.started": "2026-01-16T11:33:51.664315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "eval_loss: 0.8693\n",
      "eval_runtime: 74.3001\n",
      "eval_samples_per_second: 8.2230\n",
      "eval_steps_per_second: 2.0590\n",
      "epoch: 20.0000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate on validation set.\"\"\"\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:35:05.974365Z",
     "iopub.status.busy": "2026-01-16T11:35:05.974119Z",
     "iopub.status.idle": "2026-01-16T11:35:37.211769Z",
     "shell.execute_reply": "2026-01-16T11:35:37.211117Z",
     "shell.execute_reply.started": "2026-01-16T11:35:05.974340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "==================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "Input: Day: Tuesday, Time: 22:00 - 00:00, Score: 73, Dominance: Unrivaled, Shape: Sustained Plateau, Style: Urgent (Creating FOMO/Action)\n",
      "Expected: STRIKE NOW: Tuesday late night (22:00-00:00) crushes competitors with a 73/100 score-26 points ahead...\n",
      "Predicted: Don't miss this window! Tuesday late night (22:00-00:00) crushes competitors with a 73/100 score-nearly double the runner-up. Night owls are actively scrolling before bed, creating a sustained engagement plateau you can't afford to miss.\n",
      "\n",
      "--- Sample 2 ---\n",
      "Input: Day: Thursday, Time: 09:00 - 12:00, Score: 97, Dominance: Clear Lead, Shape: Sustained Plateau, Style: Advisory (Consultative, Helpful)\n",
      "Expected: Thursday mid-mornings (9am-12pm) capture professionals during their most active work breaks, with a ...\n",
      "Predicted: Thursday mid-morning (9 AM-12 PM) captures professionals during their first work break, delivering a commanding 97/100 engagement score-significantly outperforming alternatives. This sustained plateau period ensures consistent visibility as your audience actively checks social media between tasks.\n",
      "\n",
      "--- Sample 3 ---\n",
      "Input: Day: Tuesday, Time: 13:00 - 17:00, Score: 86, Dominance: Unrivaled, Shape: Sharp Spike, Style: Detailed (Deep dive analysis)\n",
      "Expected: Tuesday afternoon's 86/100 score with Sharp Spike behavior indicates a concentrated engagement windo...\n",
      "Predicted: Tuesday's 13:00-17:00 window demonstrates unrivaled dominance with an 86/100 score-nearly double the runner-up's 41-driven by a sharp spike in moderate work-hour activity as professionals seek content breaks during the afternoon lull. This unrivaled dominance creates optimal visibility for your content.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate sample predictions.\"\"\"\n",
    "\n",
    "\n",
    "def generate_prediction(input_text: str) -> str:\n",
    "    \"\"\"Generate prediction for a single input.\"\"\"\n",
    "    full_input = T5_PREFIX + input_text\n",
    "    inputs = tokenizer(\n",
    "        full_input,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = dataset[\"validation\"][i]\n",
    "    prediction = generate_prediction(sample[\"input_text\"])\n",
    "\n",
    "    print(f\"\\n--- Sample {i + 1} ---\")\n",
    "    print(f\"Input: {sample['input_text']}\")\n",
    "    print(f\"Expected: {sample['target_text'][:100]}...\")\n",
    "    print(f\"Predicted: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:35:37.212971Z",
     "iopub.status.busy": "2026-01-16T11:35:37.212689Z",
     "iopub.status.idle": "2026-01-16T11:35:40.304297Z",
     "shell.execute_reply": "2026-01-16T11:35:40.303674Z",
     "shell.execute_reply.started": "2026-01-16T11:35:37.212947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save merged model for inference.\"\"\"\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "MERGED_OUTPUT_DIR = f\"{OUTPUT_DIR}/merged\"\n",
    "merged_model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "\n",
    "logger.info(f\"Merged model saved to: {MERGED_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:47:21.280960Z",
     "iopub.status.busy": "2026-01-16T11:47:21.280292Z",
     "iopub.status.idle": "2026-01-16T11:47:21.286047Z",
     "shell.execute_reply": "2026-01-16T11:47:21.285239Z",
     "shell.execute_reply.started": "2026-01-16T11:47:21.280922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Skip ONNX export - model 4-bit tidak compatible dengan ONNX.\"\"\"\n",
    "\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"ONNX export skipped\")\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"Reason: 4-bit quantized models are not compatible with ONNX\")\n",
    "logger.info(\"\")\n",
    "logger.info(\"For optimized inference, use BetterTransformer:\")\n",
    "logger.info(\"  model = model.to_bettertransformer()\")\n",
    "logger.info(\"\")\n",
    "logger.info(\"Available inference options:\")\n",
    "logger.info(f\"  1. LoRA adapter: {HF_MODEL_ID}\")\n",
    "logger.info(f\"  2. Merged model: {HF_MODEL_ID}/merged\")\n",
    "\n",
    "# Set to None so upload cell knows to skip\n",
    "ONNX_OUTPUT_DIR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:49:36.902296Z",
     "iopub.status.busy": "2026-01-16T11:49:36.901755Z",
     "iopub.status.idle": "2026-01-16T11:49:36.907227Z",
     "shell.execute_reply": "2026-01-16T11:49:36.906520Z",
     "shell.execute_reply.started": "2026-01-16T11:49:36.902264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Model is ready for inference.\n",
      "\n",
      "To load the LoRA adapter model:\n",
      "\n",
      "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
      "from peft import PeftModel\n",
      "\n",
      "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
      "model = PeftModel.from_pretrained(base_model, \"raflisbk/t5-posting-time-summarizer\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"raflisbk/t5-posting-time-summarizer\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Usage instructions.\"\"\"\n",
    "\n",
    "print(\"Done! Model is ready for inference.\")\n",
    "print(f\"\\nTo load the LoRA adapter model:\")\n",
    "print(f\"\")\n",
    "print(f\"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\")\n",
    "print(f\"from peft import PeftModel\")\n",
    "print(f\"\")\n",
    "print(f'base_model = AutoModelForSeq2SeqLM.from_pretrained(\"{MODEL_NAME}\")')\n",
    "print(f'model = PeftModel.from_pretrained(base_model, \"{HF_MODEL_ID}\")')\n",
    "print(f'tokenizer = AutoTokenizer.from_pretrained(\"{HF_MODEL_ID}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:49:39.482852Z",
     "iopub.status.busy": "2026-01-16T11:49:39.482287Z",
     "iopub.status.idle": "2026-01-16T11:50:15.573077Z",
     "shell.execute_reply": "2026-01-16T11:50:15.572325Z",
     "shell.execute_reply.started": "2026-01-16T11:49:39.482822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385b59cf311340c99a5e3bd606757c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6b0307175d4eef9ee2f22dff57b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2877d6f4641ee9984b47481aeb23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f936826b8e4dd494438f76a91bf0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e0602649334059aa00381597d408fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a330604e92343a4a4304411f7f7281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save dan upload semua model ke HuggingFace Hub.\"\"\"\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "def upload_folder_to_hub(\n",
    "    local_path: str,\n",
    "    repo_id: str,\n",
    "    path_in_repo: str,\n",
    "    token: str\n",
    ") -> None:\n",
    "    \"\"\"Upload folder ke HuggingFace Hub.\n",
    "\n",
    "    Args:\n",
    "        local_path: Path lokal folder yang akan di-upload.\n",
    "        repo_id: ID repository di Hub.\n",
    "        path_in_repo: Path tujuan di dalam repository.\n",
    "        token: HuggingFace API token.\n",
    "    \"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    api.upload_folder(\n",
    "        folder_path=local_path,\n",
    "        repo_id=repo_id,\n",
    "        path_in_repo=path_in_repo,\n",
    "        token=token,\n",
    "    )\n",
    "    logger.info(f\"Uploaded {local_path} to {repo_id}/{path_in_repo}\")\n",
    "\n",
    "\n",
    "# Save model locally first\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 1. Upload LoRA adapter (root)\n",
    "logger.info(\"Uploading LoRA adapter...\")\n",
    "trainer.push_to_hub(commit_message=\"Final LoRA adapter\", blocking=True)\n",
    "\n",
    "# 2. Upload merged model\n",
    "logger.info(\"Uploading merged model...\")\n",
    "upload_folder_to_hub(\n",
    "    local_path=MERGED_OUTPUT_DIR,\n",
    "    repo_id=HF_MODEL_ID,\n",
    "    path_in_repo=\"merged\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# 3. Upload ONNX model (jika ada)\n",
    "if ONNX_OUTPUT_DIR and os.path.exists(ONNX_OUTPUT_DIR):\n",
    "    logger.info(\"Uploading ONNX model...\")\n",
    "    upload_folder_to_hub(\n",
    "        local_path=ONNX_OUTPUT_DIR,\n",
    "        repo_id=HF_MODEL_ID,\n",
    "        path_in_repo=\"onnx\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"ONNX model not available, skipping upload\")\n",
    "    \n",
    "logger.info(\"All models uploaded successfully!\")\n",
    "logger.info(f\"Repository: https://huggingface.co/{HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Opsi 1: LoRA adapter (lebih kecil)\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "model = PeftModel.from_pretrained(base_model, \"raflisbk/t5-posting-time-summarizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"raflisbk/t5-posting-time-summarizer\")\n",
    "\n",
    "# Opsi 2: Merged model (lebih cepat load)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"raflisbk/t5-posting-time-summarizer\",\n",
    "    subfolder=\"merged\"\n",
    ")\n",
    "\n",
    "# Optimasi inference\n",
    "model = model.to_bettertransformer()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9263905,
     "sourceId": 14504412,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
