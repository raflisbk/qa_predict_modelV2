{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14504412,"sourceType":"datasetVersion","datasetId":9263905},{"sourceId":14518696,"sourceType":"datasetVersion","datasetId":9272824},{"sourceId":14553595,"sourceType":"datasetVersion","datasetId":9295586}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Setup & Installation","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate bitsandbytes huggingface_hub rouge-score","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:33.019283Z","iopub.execute_input":"2026-01-20T03:34:33.019588Z","iopub.status.idle":"2026-01-20T03:34:36.666786Z","shell.execute_reply.started":"2026-01-20T03:34:33.019555Z","shell.execute_reply":"2026-01-20T03:34:36.665991Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport logging\nimport torch\nfrom pathlib import Path\n\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom huggingface_hub import login, HfApi\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:36.668035Z","iopub.execute_input":"2026-01-20T03:34:36.668349Z","iopub.status.idle":"2026-01-20T03:34:46.018513Z","shell.execute_reply.started":"2026-01-20T03:34:36.668308Z","shell.execute_reply":"2026-01-20T03:34:46.017866Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2026-01-20 03:34:42.660026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768880082.681294     235 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768880082.687797     235 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768880082.704702     235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768880082.704722     235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768880082.704724     235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768880082.704726     235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"GPU Available: True\nGPU: Tesla T4\nMemory: 15.83 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")\nHF_USERNAME = \"raflisbk\"  \nHF_REPO_NAME = \"t5-posting-time-summarizer\"\n  \nSTAGE1_MODEL_PATH = f\"{HF_USERNAME}/{HF_REPO_NAME}\"\nSTAGE1_SUBFOLDER = \"stage1\"\n\ndf = pd.read_csv('/kaggle/input/stage-2-narrative/stage2_training_narrative.csv')\n\nMAX_INPUT_LENGTH = 256  \nMAX_TARGET_LENGTH = 1024 \n\nLORA_R = 64\nLORA_ALPHA = 128\nLORA_DROPOUT = 0.05\nLORA_TARGET_MODULES = [\"q\", \"v\", \"k\", \"o\"]\n\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 0.0001   \nNUM_EPOCHS = 10\nWARMUP_RATIO = 0.15\n\nOUTPUT_DIR = \"./outputs_stage2b\"\nMERGED_OUTPUT_DIR = \"../models/stage2_mergedb\"","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:46.019501Z","iopub.execute_input":"2026-01-20T03:34:46.020052Z","iopub.status.idle":"2026-01-20T03:34:46.132131Z","shell.execute_reply.started":"2026-01-20T03:34:46.020016Z","shell.execute_reply":"2026-01-20T03:34:46.131377Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"Logged in to HuggingFace Hub\")\nelse:\n    print(\"Warning: HF_TOKEN not set. Set it with: os.environ['HF_TOKEN'] = 'your_token'\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:46.133158Z","iopub.execute_input":"2026-01-20T03:34:46.133389Z","iopub.status.idle":"2026-01-20T03:34:46.346042Z","shell.execute_reply.started":"2026-01-20T03:34:46.133367Z","shell.execute_reply":"2026-01-20T03:34:46.345533Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Logged in to HuggingFace Hub\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Load Data","metadata":{}},{"cell_type":"code","source":"print(f\"Loaded {len(df)} samples\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE INPUT:\")\nprint(df.iloc[0]['input_text'])\nprint(\"\\nSAMPLE TARGET:\")\nprint(df.iloc[0]['target_text'])\nprint(\"=\"*60)","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:46.348220Z","iopub.execute_input":"2026-01-20T03:34:46.348459Z","iopub.status.idle":"2026-01-20T03:34:46.353561Z","shell.execute_reply.started":"2026-01-20T03:34:46.348437Z","shell.execute_reply":"2026-01-20T03:34:46.352755Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loaded 321 samples\n\n============================================================\nSAMPLE INPUT:\nDay: Sunday, Time: 15:00 - 18:00, Score: 82\nHourly: 01(67), 01(39), 02(39), 02(47), 03(75), 03(25), 04(34), 04(42), 05(53), 05(48), 06(66), 06(52), 07(75), 07(47), 08(72), 08(67), 09(72), 09(80), 10(70), 10(98), 11(88), 11(75), 12(77), 12(67), 13(81), 13(71), 14(83), 14(72), 15(86), 15(93), 16(81), 16(81), 17(71), 17(81), 17(80), 18(74), 18(83), 18(69), 19(69), 19(80), 19(80), 20(76), 20(73), 20(81), 21(62), 21(72), 21(73), 22(64), 22(67), 22(68), 23(62), 23(51), 23(60)\nDaily Avg: 68.3, Peak: 15(93)\n\nSAMPLE TARGET:\nSunday from 3 PM to 6 PM is a particularly strong engagement window, likely catching users as they wind down their weekend afternoons.\n\nInsight:\n- The highest hourly engagement occurs at 3 PM, peaking at 93.\n- Engagement during this period is 20% higher than the daily average, signalling increased user attention.\n- Even the lowest hourly scores within this window comfortably exceed the daily average of 68.3.\n============================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset\n})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:46.354345Z","iopub.execute_input":"2026-01-20T03:34:46.354611Z","iopub.status.idle":"2026-01-20T03:34:46.391260Z","shell.execute_reply.started":"2026-01-20T03:34:46.354574Z","shell.execute_reply":"2026-01-20T03:34:46.390694Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train: 288, Val: 33\nDatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 288\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 33\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 4. Load Stage 1 Model & Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    STAGE1_MODEL_PATH,\n    subfolder=STAGE1_SUBFOLDER\n)\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float32,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    STAGE1_MODEL_PATH,\n    subfolder=STAGE1_SUBFOLDER,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(f\"Model loaded from: {STAGE1_MODEL_PATH}/{STAGE1_SUBFOLDER}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:34:46.392047Z","iopub.execute_input":"2026-01-20T03:34:46.392423Z","iopub.status.idle":"2026-01-20T03:35:03.848432Z","shell.execute_reply.started":"2026-01-20T03:34:46.392401Z","shell.execute_reply":"2026-01-20T03:35:03.847851Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Tokenizer loaded: T5TokenizerFast\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942ddb7c72604318ab8fcbecfcc676c9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:1598: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d01ae5cd298483f9a03e72c950dd09c"}},"metadata":{}},{"name":"stdout","text":"Model loaded from: raflisbk/t5-posting-time-summarizer/stage1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    target_modules=LORA_TARGET_MODULES,\n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:35:03.849329Z","iopub.execute_input":"2026-01-20T03:35:03.849983Z","iopub.status.idle":"2026-01-20T03:35:04.738787Z","shell.execute_reply.started":"2026-01-20T03:35:03.849943Z","shell.execute_reply":"2026-01-20T03:35:04.738113Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 37,748,736 || all params: 820,898,816 || trainable%: 4.5985\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 5. Tokenization","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [f\"summarize: {text}\" for text in examples[\"input_text\"]]\n    targets = examples[\"target_text\"]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    labels = tokenizer(\n        targets,\n        max_length=MAX_TARGET_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    labels[\"input_ids\"] = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n        for labels_seq in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\nprint(f\"Tokenized dataset: {tokenized_dataset}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:35:04.739813Z","iopub.execute_input":"2026-01-20T03:35:04.740186Z","iopub.status.idle":"2026-01-20T03:35:06.125812Z","shell.execute_reply.started":"2026-01-20T03:35:04.740162Z","shell.execute_reply":"2026-01-20T03:35:06.125194Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/288 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17eaf0869f5f4ef1afa306566480cb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/33 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6cca6ae260a4646981674672b4118f8"}},"metadata":{}},{"name":"stdout","text":"Tokenized dataset: DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 288\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 33\n    })\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 6. Training Setup","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True\n)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    \n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    \n    num_train_epochs=NUM_EPOCHS,\n    \n    fp16=False,  \n    bf16=False,\n    \n    optim=\"paged_adamw_8bit\",\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    \n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n\n    logging_steps=10,\n    report_to=[],\n\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LENGTH,\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    push_to_hub=True,\n    hub_model_id = STAGE1_MODEL_PATH,\n    hub_token=HF_TOKEN\n)\n\nprint(\"Training arguments configured!\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:35:06.126606Z","iopub.execute_input":"2026-01-20T03:35:06.126894Z","iopub.status.idle":"2026-01-20T03:35:06.161233Z","shell.execute_reply.started":"2026-01-20T03:35:06.126870Z","shell.execute_reply":"2026-01-20T03:35:06.160517Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training arguments configured!\nEffective batch size: 16\nLearning rate: 0.0001\nEpochs: 10\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer initialized!\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:35:06.162219Z","iopub.execute_input":"2026-01-20T03:35:06.162639Z","iopub.status.idle":"2026-01-20T03:35:06.431656Z","shell.execute_reply.started":"2026-01-20T03:35:06.162615Z","shell.execute_reply":"2026-01-20T03:35:06.431064Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_235/1801678902.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Trainer initialized!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 7. Train!","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2026-01-20T03:35:06.432428Z","iopub.execute_input":"2026-01-20T03:35:06.432702Z","iopub.status.idle":"2026-01-20T04:58:43.549645Z","shell.execute_reply.started":"2026-01-20T03:35:06.432679Z","shell.execute_reply":"2026-01-20T04:58:43.548852Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 1:23:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.286300</td>\n      <td>2.759873</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.788700</td>\n      <td>2.089126</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.166000</td>\n      <td>1.719419</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.854500</td>\n      <td>1.558257</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.734300</td>\n      <td>1.482579</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.668700</td>\n      <td>1.430487</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.658900</td>\n      <td>1.401300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.587000</td>\n      <td>1.386422</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.562600</td>\n      <td>1.381265</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.574900</td>\n      <td>1.380317</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=180, training_loss=1.975002776251899, metrics={'train_runtime': 5011.9157, 'train_samples_per_second': 0.575, 'train_steps_per_second': 0.036, 'total_flos': 3485854177689600.0, 'train_loss': 1.975002776251899, 'epoch': 10.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 8. Evaluation","metadata":{}},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(f\"\\nEvaluation Results:\")\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T05:33:11.160900Z","iopub.execute_input":"2026-01-20T05:33:11.161683Z","iopub.status.idle":"2026-01-20T05:33:29.667791Z","shell.execute_reply.started":"2026-01-20T05:33:11.161653Z","shell.execute_reply":"2026-01-20T05:33:29.667176Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nEvaluation Results:\n  eval_loss: 1.3803\n  eval_runtime: 18.4974\n  eval_samples_per_second: 1.7840\n  eval_steps_per_second: 0.4870\n  epoch: 10.0000\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"test_input = val_df.iloc[0]['input_text']\nexpected_output = val_df.iloc[0]['target_text']\n\nprint(\"INPUT:\")\nprint(test_input)\n\ninputs = tokenizer(f\"summarize: {test_input}\", return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True)\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_length=MAX_TARGET_LENGTH,\n        num_beams=4,\n        early_stopping=True,\n        do_sample=False\n    )\n\ngenerated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"\\nEXPECTED:\")\nprint(expected_output)\n\nprint(\"\\nGENERATED:\")\nprint(generated)","metadata":{"execution":{"iopub.status.busy":"2026-01-20T05:33:29.669112Z","iopub.execute_input":"2026-01-20T05:33:29.669346Z","iopub.status.idle":"2026-01-20T05:33:45.531534Z","shell.execute_reply.started":"2026-01-20T05:33:29.669323Z","shell.execute_reply":"2026-01-20T05:33:45.530917Z"},"trusted":true},"outputs":[{"name":"stdout","text":"INPUT:\nDay: Saturday, Time: 09:00 - 12:00, Score: 79\nHourly: 01(30), 01(37), 02(28), 02(32), 03(30), 03(35), 04(37), 04(33), 05(44), 05(50), 06(48), 06(59), 07(65), 07(69), 08(84), 08(89), 09(97), 09(80), 10(84), 10(71), 11(77), 11(66), 12(73), 12(57), 13(77), 13(63), 14(74), 14(65), 15(59), 15(71), 16(63), 16(58), 17(59), 17(53), 18(50), 18(55), 19(54), 19(46), 20(50), 20(43), 21(50), 21(44), 22(43), 22(38), 23(34), 23(38)\nDaily Avg: 55.7, Peak: 09(97)\n\nEXPECTED:\nSaturday's 09:00-12:00 slot is ideal for engagement, capturing users during their active weekend morning with a strong 79/100 score.\n\nInsight:\n- The peak engagement hits 97 at 9 AM, soaring 74% above the daily average of 55.7.\n- This morning window outperforms the afternoon (1-4 PM) by 12%, showing higher user receptivity.\n- Sustained high scores from 8-10 AM indicate a prime window for priority content.\n\nGENERATED:\nSaturday's 09:00-12:00 slot delivers a strong 79/100 engagement score, capitalizing on peak morning activity when users are most active and receptive. Insight: - The 9 AM hour hits 97 points, which is 33% higher than the daily average of 55.7. - Engagement remains consistently high throughout the 9 AM hour, with a peak of 97 at 9 AM. - This window outperforms the late afternoon (11 AM-12 PM) by 33%, making it a prime time for content.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 9. Save & Merge Model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"LoRA adapter saved to: {OUTPUT_DIR}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T05:37:55.684159Z","iopub.execute_input":"2026-01-20T05:37:55.684712Z","iopub.status.idle":"2026-01-20T05:37:59.604033Z","shell.execute_reply.started":"2026-01-20T05:37:55.684684Z","shell.execute_reply":"2026-01-20T05:37:59.603453Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e9a11d8952941b9b520bd6e33c2b865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742ab87285ed43229709303a4e21c553"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"LoRA adapter saved to: ./outputs_stage2b\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 10. Upload to HuggingFace Hub","metadata":{}},{"cell_type":"code","source":"print(\"Uploading LoRA adapter to HuggingFace Hub...\")\n\nfrom huggingface_hub import HfApi\n\n# Upload LoRA adapter langsung (tanpa merge)\napi = HfApi(token=HF_TOKEN)\n\nprint(f\"Uploading adapter from {OUTPUT_DIR} to {STAGE1_MODEL_PATH}/stage2...\")\napi.upload_folder(\n    folder_path=OUTPUT_DIR,\n    repo_id=STAGE1_MODEL_PATH,\n    path_in_repo=\"stage2b\",\n    token=HF_TOKEN\n)\n\nprint(f\" Stage 2 LoRA adapter uploaded to: {STAGE1_MODEL_PATH}/stage2b\")\nprint(\"\\nTo use this model for inference:\")\nprint(\"1. Load base model from stage1\")\nprint(\"2. Load LoRA adapter from stage2\")\nprint(\"3. Merge at inference time (in float16, not 4bit)\")","metadata":{"execution":{"iopub.status.busy":"2026-01-20T05:40:50.564919Z","iopub.execute_input":"2026-01-20T05:40:50.565721Z","iopub.status.idle":"2026-01-20T05:41:11.391299Z","shell.execute_reply.started":"2026-01-20T05:40:50.565689Z","shell.execute_reply":"2026-01-20T05:41:11.390451Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Uploading LoRA adapter to HuggingFace Hub...\nUploading adapter from ./outputs_stage2b to raflisbk/t5-posting-time-summarizer/stage2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5395f08f5e94bd888e50ba1841f5ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0258c5d5a15a4cf2beb37385f001276e"}},"metadata":{}},{"name":"stdout","text":" Stage 2 LoRA adapter uploaded to: raflisbk/t5-posting-time-summarizer/stage2b\n\nTo use this model for inference:\n1. Load base model from stage1\n2. Load LoRA adapter from stage2\n3. Merge at inference time (in float16, not 4bit)\n","output_type":"stream"}],"execution_count":22}]}