{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Trends Prediction - LSTM Model\n",
    "\n",
    "This notebook trains an LSTM (Long Short-Term Memory) deep learning model to predict daily interest values.\n",
    "\n",
    "**Algorithm**: LSTM Neural Network\n",
    "- Captures long-term dependencies in time series\n",
    "- Handles sequential data naturally\n",
    "- Good for complex patterns\n",
    "\n",
    "**Target**: Predict interest_value (0-100) for next N days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed daily trends data\n",
    "data_path = '../data/processed/daily_trends_processed_latest.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['keyword', 'category', 'date'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Keywords: {df['keyword'].nunique()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training\n",
    "    \n",
    "    Args:\n",
    "        data: Time series data\n",
    "        lookback: Number of past days to use\n",
    "        forecast_horizon: Number of future days to predict\n",
    "    \n",
    "    Returns:\n",
    "        X, y arrays for training\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "LOOKBACK = 30  # Use 30 days of history\n",
    "FORECAST_HORIZON = 7  # Predict next 7 days\n",
    "\n",
    "print(f\"Sequence parameters:\")\n",
    "print(f\"  Lookback: {LOOKBACK} days\")\n",
    "print(f\"  Forecast horizon: {FORECAST_HORIZON} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for a specific keyword/category\n",
    "# For simplicity, we'll train one model per keyword-category combination\n",
    "# In production, you might want to train a single model for all\n",
    "\n",
    "sample_keyword = df['keyword'].iloc[0]\n",
    "sample_category = df['category'].iloc[0]\n",
    "\n",
    "# Filter data\n",
    "keyword_data = df[\n",
    "    (df['keyword'] == sample_keyword) & \n",
    "    (df['category'] == sample_category)\n",
    "].copy()\n",
    "\n",
    "keyword_data = keyword_data.sort_values('date')\n",
    "\n",
    "print(f\"Training model for: {sample_keyword} ({sample_category})\")\n",
    "print(f\"Data points: {len(keyword_data)}\")\n",
    "\n",
    "# Extract interest values\n",
    "values = keyword_data['interest_value'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "print(f\"Data normalized to range [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "X, y = create_sequences(scaled_values, LOOKBACK, FORECAST_HORIZON)\n",
    "\n",
    "print(f\"Sequence shapes:\")\n",
    "print(f\"  X: {X.shape} (samples, lookback, features)\")\n",
    "print(f\"  y: {y.shape} (samples, forecast_horizon, features)\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} sequences\")\n",
    "print(f\"Test set: {X_test.shape[0]} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(lookback, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Build LSTM model architecture\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First LSTM layer\n",
    "        LSTM(128, return_sequences=True, input_shape=(lookback, 1)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(forecast_horizon)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_lstm_model(LOOKBACK, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    '../models/lstm_daily_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Reshape y for training (flatten forecast horizon)\n",
    "y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
    "y_test_flat = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# Train model\n",
    "print(\"Training LSTM model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_flat,\n",
    "    validation_data=(X_test, y_test_flat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Model MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_train_inv = scaler.inverse_transform(y_train_flat)\n",
    "y_test_inv = scaler.inverse_transform(y_test_flat)\n",
    "y_pred_train_inv = scaler.inverse_transform(y_pred_train)\n",
    "y_pred_test_inv = scaler.inverse_transform(y_pred_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R2:   {r2:.4f}\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train_inv, y_pred_train_inv, \"Train\")\n",
    "test_metrics = calculate_metrics(y_test_inv, y_pred_test_inv, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for test set\n",
    "# Show first 5 test sequences\n",
    "n_samples = min(5, len(y_test_inv))\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 1, figsize=(14, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    axes[i].plot(range(FORECAST_HORIZON), y_test_inv[i], \n",
    "                label='Actual', marker='o', linewidth=2)\n",
    "    axes[i].plot(range(FORECAST_HORIZON), y_pred_test_inv[i], \n",
    "                label='Predicted', marker='s', linewidth=2, alpha=0.7)\n",
    "    axes[i].set_title(f'Test Sample {i+1}')\n",
    "    axes[i].set_xlabel('Days Ahead')\n",
    "    axes[i].set_ylabel('Interest Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted (all forecast horizons)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Train set\n",
    "axes[0].scatter(y_train_inv.flatten(), y_pred_train_inv.flatten(), alpha=0.3, s=10)\n",
    "axes[0].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Interest Value')\n",
    "axes[0].set_ylabel('Predicted Interest Value')\n",
    "axes[0].set_title(f'Train Set (R2 = {train_metrics[\"R2\"]:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test_inv.flatten(), y_pred_test_inv.flatten(), alpha=0.3, s=10, color='orange')\n",
    "axes[1].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Interest Value')\n",
    "axes[1].set_ylabel('Predicted Interest Value')\n",
    "axes[1].set_title(f'Test Set (R2 = {test_metrics[\"R2\"]:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved via ModelCheckpoint callback\n",
    "print(f\"Model saved to: ../models/lstm_daily_model.keras\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/lstm_scaler.pkl')\n",
    "print(f\"Scaler saved to: ../models/lstm_scaler.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "import json\n",
    "metrics = {\n",
    "    'train': train_metrics,\n",
    "    'test': test_metrics,\n",
    "    'lookback': LOOKBACK,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'keyword': sample_keyword,\n",
    "    'category': sample_category\n",
    "}\n",
    "\n",
    "with open('../models/lstm_daily_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to: ../models/lstm_daily_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Model Performance:\n",
    "- Algorithm: LSTM Neural Network\n",
    "- Lookback: 30 days\n",
    "- Forecast Horizon: 7 days\n",
    "- Test RMSE: Check output above\n",
    "- Test R2: Check output above\n",
    "\n",
    "### Advantages:\n",
    "- Captures long-term dependencies\n",
    "- Handles sequential patterns well\n",
    "- Multi-step forecasting\n",
    "\n",
    "### Limitations:\n",
    "- Requires more data than traditional ML\n",
    "- Longer training time\n",
    "- Needs careful hyperparameter tuning\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different architectures (GRU, Bidirectional LSTM)\n",
    "2. Add attention mechanism\n",
    "3. Experiment with different lookback windows\n",
    "4. Train separate models for each keyword/category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
